from dotenv import load_dotenv
from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel
from apps.langchain_bot.dependencies import document_retriever, llm
from typing import Dict, Any


def get_context(inputs: Dict[str, Any]) -> str:
    """
    Retrieves the top k tables based on the user's query and formats them as a string.

    Args:
        inputs (Dict[str, Any]): A dictionary containing the user's query under the key "question".

    Returns:
        str: A formatted string representation of the top k tables related to the query.
    """
    context = document_retriever.find_top_k_similar(inputs["question"], k=3)
    return context


def create_final_prompt(question: str, context: str) -> str:
    """
    Creates the final prompt by combining the user's question and the retrieved context.

    Args:
        question (str): The user's question.
        context (str): The formatted context retrieved based on the question.

    Returns:
        str: A complete prompt including the question and context.
    """
    return f"Question : {question} \n Context : {context}"


def generate_sql_llm(final_prompt: str) -> str:
    """
    Generates SQL code using the provided final prompt.

    Args:
        final_prompt (str): The complete prompt including the question and context.

    Returns:
        str: The SQL query generated by the LLM.
    """
    return llm.invoke(final_prompt)


def get_query_results(sql_query: str) -> Any:
    """
    Executes the provided SQL query and returns the results.

    Args:
        sql_query (str): The SQL query to execute.

    Returns:
        str: The results of the query (mocked as "25 students").
    """
    print(sql_query)
    return "25 students"


def rephrase_query_results(query_results: Any) -> str:
    """
    Rephrases the query results into a user-friendly answer.

    Args:
        query_results (str): The raw results of the query.

    Returns:
        str: A user-friendly rephrased answer (mocked as "friendly answer").
    """
    print(query_results)
    return "friendly answer"


# Wrap functions with RunnableLambda
get_context_runnable = RunnableLambda(get_context)
create_final_prompt_runnable = RunnableLambda(lambda inputs: create_final_prompt(**inputs))
generate_sql_llm_runnable = RunnableLambda(generate_sql_llm)
get_query_results_runnable = RunnableLambda(get_query_results)
rephrase_query_results_runnable = RunnableLambda(rephrase_query_results)

# Compose the chain using the pipe operator
chain = (
        RunnableParallel({
            "context": get_context_runnable,
            "question": RunnablePassthrough(),
        })
        | create_final_prompt_runnable
        | generate_sql_llm_runnable
        | get_query_results_runnable
        | rephrase_query_results_runnable
)

# Main execution
if __name__ == '__main__':
    load_dotenv()
    result = chain.invoke({"question": "how many students at Boston?", "messages": []})
    print(result)
